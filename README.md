This research addresses the pivotal task of developing a cutting-edge **Personalized Travel Recommendation System (PTRS)**, aiming to enhance the travel experience through tailored recommendations. Leveraging social network data from **Gowalla**, we intricately capture user behavior via key attributes like **user ID**, **timestamp**, and **location** coordinates. Our study explores traditional sequential models such as **LSTM** and **Bi-LSTM**, known for temporal pattern capturing, and compares them with
transformer models, particularly **BERT**, renowned for natural language processing. In evaluating precision, recall, F1-score, and training times, our results reveal a nuanced performance landscape. While non-transformer models excel in precision and recall, transformer models, notably BERT, exhibit consistent loss reduction and overall robustness, outperforming counterparts in stability and effective convergence. By addressing the limitations of existing approaches and showcasing compelling experimental results, this research pioneers a transformative fusion of transformer and non-transformer models for personalized travel recommendations, marking a significant stride in the evolution of location-based recommender systems.
